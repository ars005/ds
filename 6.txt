import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# --- 0. Data Generation (Simulate a dataset with 4 features) ---
np.random.seed(42)
N_samples = 300
# Create features where X1 and X2 are correlated, and X3 and X4 are correlated
X1 = np.random.normal(10, 3, N_samples)
X2 = X1 * 0.8 + np.random.normal(0, 1, N_samples)
X3 = np.random.normal(50, 5, N_samples)
X4 = X3 * 1.5 + np.random.normal(0, 2, N_samples)
Target = np.random.randint(0, 3, N_samples) # Dummy target for visualization

data = pd.DataFrame({
    'Feature_A': X1,
    'Feature_B': X2,
    'Feature_C': X3,
    'Feature_D': X4
})
data['Target'] = Target

print("--- Original Data Head (4 Features + Target) ---")
print(data.head())
print(f"Original shape: {data.shape}\n")

# --- 1. Data Preparation: Scaling ---
# PCA is highly sensitive to scale, so scaling is mandatory.
features = ['Feature_A', 'Feature_B', 'Feature_C', 'Feature_D']
X = data[features]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=features)

print("--- Scaled Data Head ---")
print(X_scaled_df.head())
print("\n" + "="*50 + "\n")

# --- 2. Implement PCA and Determine Components ---

# Instantiate PCA (We will keep all 4 components initially)
pca = PCA()
pca.fit(X_scaled)

# 2.1 Explained Variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# Plotting the Explained Variance
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance, marker='o', linestyle='--', label='Cumulative Explained Variance')
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center', label='Individual Explained Variance')
plt.title('Scree Plot: Explained Variance by Principal Component')
plt.xlabel('Principal Component Index')
plt.ylabel('Explained Variance Ratio')
plt.xticks(range(1, len(explained_variance_ratio) + 1))
plt.legend()
plt.grid(True)
plt.show() 

print("Explained Variance Ratio:")
for i, ratio in enumerate(explained_variance_ratio):
    print(f"PC{i+1}: {ratio:.4f}")
print(f"Cumulative Variance explained by PC1 and PC2: {cumulative_variance[1]:.4f}")
print("\n" + "="*50 + "\n")

# --- 3. Transform Data (Dimensionality Reduction) ---

# Based on the plot, PC1 and PC2 capture most of the variance (e.g., > 90%).
# We choose n_components = 2 for visualization.
n_components = 2
pca_final = PCA(n_components=n_components)
X_pca = pca_final.fit_transform(X_scaled)

# Convert back to DataFrame
pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(n_components)])
pca_df['Target'] = data['Target']

print("--- Transformed Data Head (2 Principal Components) ---")
print(pca_df.head())
print(f"New shape: {pca_df.shape}\n")

# --- 4. Visualization of Transformed Data ---
plt.figure(figsize=(8, 6))
scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=pca_df['Target'], cmap='viridis', s=50, alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Data Projected onto the First Two Principal Components')
plt.colorbar(scatter, ticks=np.unique(pca_df['Target']), label='Target Class')
plt.grid(True)
plt.show()
